{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38e5d6e4-5315-4e7d-856e-809bd073fdfd",
    "_uuid": "cf776811-bfa0-4ef8-8729-d3548786bf28",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# ðŸŒ¿ Lemon Tree Leaf Pre-Processing (Background Removal + CLAHE + Filter + Gamma)\n",
    "\n",
    "Download the dataset from this link: https://www.kaggle.com/datasets/mahmoudshaheen1134/lemon-leaf-disease-dataset-lldd/data\n",
    "and rename the folder to be `Original Dataset`.\n",
    "\n",
    "Final pipeline **order**:\n",
    "1. **Read original** (BGR)\n",
    "2. **Resize** to 256 Ã— 256\n",
    "3. **Background removal** (GrabCut + white fill)\n",
    "4. **BGR â†’ L*a*b***\n",
    "5. **CLAHE** on the *L* channel only\n",
    "6. **Merge** L-A-B\n",
    "7. **Noise-filter** (choose AMF Â· MF Â· TVF Â· GDF)\n",
    "8. **Gamma correction**\n",
    "9. **Save** (RGB stored as PNG)\n",
    " \n",
    "Directory mirror:\n",
    "```text\n",
    "./Original Dataset/<Disease>/<img>.png â†’ ./Processed dataset/bg_clahe_<filter>/<Disease>/<img>.png\n",
    "```\n",
    "\n",
    "Install:\n",
    "```bash\n",
    "pip install opencv-python scikit-image numpy tqdm\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "296da9f9-4e1c-44d1-9094-26911f612b09",
    "_uuid": "bfdb398b-4fb0-4224-9907-687528417a28",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports & configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.restoration import denoise_tv_chambolle   # Totalâ€‘variation\n",
    "from skimage.filters.rank import median as sk_median     # Adaptive median\n",
    "from skimage.morphology import disk\n",
    "\n",
    "# Paths\n",
    "INPUT_ROOT  = Path(\"./Original Dataset\")\n",
    "OUTPUT_ROOT = Path(\"./Processed dataset\")\n",
    "\n",
    "# Resize target\n",
    "RESIZE_WH = (256, 256)  # (w, h)\n",
    "\n",
    "# Hyperâ€‘parameters\n",
    "CLAHE_PARAMS = {\"clipLimit\": 2.0, \"tileGridSize\": (8, 8)}\n",
    "GAMMA_PARAMS = {\"gamma\": 1.2}  # >Â 1 brightens, <Â 1 darkens\n",
    "\n",
    "AMF_PARAMS = {\"selem_size\": 3}         # Adaptive Median\n",
    "MF_PARAMS  = {\"ksize\": 3}              # Median blur\n",
    "TVF_PARAMS = {\"weight\": 0.1, \"eps\": 1e-3}  # Total Variation\n",
    "GDF_PARAMS = {\"ksize\": (3, 3), \"sigma\": 0} # Gaussian blur\n",
    "\n",
    "# Decide which filters to run\n",
    "FILTER_KEYS: List[str] = [\"amf\", \"mf\", \"tvf\", \"gdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35d87f8e-c85c-4984-80a5-03547453d61c",
    "_uuid": "ea4b6da4-162f-4de9-9a1d-f5c21d46ae95",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1 Â· Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "4e71a0e0-8647-4219-b0e1-f6dc5a966c4e",
    "_uuid": "d7dbc7f5-f5e6-48b2-ac6c-36cba9921c6d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Background removal\n",
    "\n",
    "def remove_background(img_bgr: np.ndarray, margin: float = 0.05, iterations: int = 5) -> np.ndarray:\n",
    "    \"\"\"GrabCut with rectangular init; nonâ€‘leaf pixels painted white.\"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    rect = (\n",
    "        int(margin * w),\n",
    "        int(margin * h),\n",
    "        int((1 - 2 * margin) * w),\n",
    "        int((1 - 2 * margin) * h),\n",
    "    )\n",
    "    mask = np.zeros((h, w), np.uint8)\n",
    "    bgdModel = np.zeros((1, 65), np.float64)\n",
    "    fgdModel = np.zeros((1, 65), np.float64)\n",
    "    cv2.grabCut(img_bgr, mask, rect, bgdModel, fgdModel, iterations, cv2.GC_INIT_WITH_RECT)\n",
    "    mask_fg = np.where((mask == 2) | (mask == 0), 0, 1).astype(\"uint8\")\n",
    "    white = np.full_like(img_bgr, 255)\n",
    "    return np.where(mask_fg[:, :, None] == 0, white, img_bgr)\n",
    "\n",
    "# Filters\n",
    "\n",
    "def adaptive_median(img_bgr: np.ndarray, selem_size: int = 3) -> np.ndarray:\n",
    "    footprint = disk(selem_size)\n",
    "    out_ch = [sk_median(ch, footprint=footprint) for ch in cv2.split(img_bgr)]\n",
    "    return cv2.merge(out_ch)\n",
    "\n",
    "\n",
    "def median_filter(img_bgr: np.ndarray, ksize: int = 3) -> np.ndarray:\n",
    "    return cv2.medianBlur(img_bgr, ksize)\n",
    "\n",
    "\n",
    "def total_variation(img_bgr: np.ndarray, weight: float = 0.1, eps: float = 1e-3) -> np.ndarray:\n",
    "    float_img = img_bgr.astype(np.float32) / 255.0\n",
    "    den = denoise_tv_chambolle(float_img, weight=weight, eps=eps, channel_axis=-1)\n",
    "    return (den * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def gaussian_denoise(img_bgr: np.ndarray, ksize: Tuple[int, int] = (3, 3), sigma: float = 0):\n",
    "    return cv2.GaussianBlur(img_bgr, ksize, sigma)\n",
    "\n",
    "FILTER_FUNCS = {\n",
    "    \"amf\": (adaptive_median, AMF_PARAMS),\n",
    "    \"mf\":  (median_filter,   MF_PARAMS),\n",
    "    \"tvf\": (total_variation, TVF_PARAMS),\n",
    "    \"gdf\": (gaussian_denoise, GDF_PARAMS),\n",
    "}\n",
    "\n",
    "# Gamma correction\n",
    "\n",
    "def gamma_correction(img_rgb: np.ndarray, gamma: float = 1.2) -> np.ndarray:\n",
    "    invG = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** invG * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img_rgb, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3c886ea-2216-4963-82a8-be7854a0c603",
    "_uuid": "c540da03-90d7-45d6-9648-040b67c8be20",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 2 Â· Core pipeline for **one** image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "096c8259-08a4-4a22-8a00-a79c99690598",
    "_uuid": "2f9d2750-982a-4847-be6c-d076e5a3bd2c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_image(img_path: Path, filter_key: str):\n",
    "    img_bgr = cv2.imread(str(img_path))\n",
    "    if img_bgr is None:\n",
    "        print(f\"[Warning] {img_path} unreadable\")\n",
    "        return\n",
    "\n",
    "    # Resize\n",
    "    img_bgr = cv2.resize(img_bgr, RESIZE_WH, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Background removal\n",
    "    img_bgr = remove_background(img_bgr)\n",
    "\n",
    "    # BGR â†’ Lab & CLAHE on L\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l_eq = cv2.createCLAHE(**CLAHE_PARAMS).apply(l)\n",
    "    img_bgr = cv2.cvtColor(cv2.merge([l_eq, a, b]), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # Noise filter\n",
    "    func, params = FILTER_FUNCS[filter_key]\n",
    "    img_bgr = func(img_bgr, **params)\n",
    "\n",
    "    # Gamma correction in RGB\n",
    "    img_rgb = gamma_correction(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), **GAMMA_PARAMS)\n",
    "\n",
    "    # Save\n",
    "    out_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "    rel = img_path.relative_to(INPUT_ROOT)\n",
    "    out_dir = OUTPUT_ROOT / f\"no_bg_clahe_{filter_key}\" / rel.parent\n",
    "    ensure_dir(out_dir)\n",
    "    cv2.imwrite(str(out_dir / rel.name), out_bgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b33b393-d439-4bea-a8d2-623c4542e87f",
    "_uuid": "ed92b1ed-7766-4a27-9d9c-37b16ed0ef8a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 3 Â· Run over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "2aed2e37-a8dc-4964-a6d1-7fc2681c679e",
    "_uuid": "8578dab8-7e78-4eea-ae5c-481c94e6a746",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1354 images in Original Dataset\n",
      "\n",
      "â–¶ï¸Ž Running: CLAHE + AMF filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clahe_amf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1354/1354 [05:08<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ï¸Ž Running: CLAHE + MF filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clahe_mf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1354/1354 [04:39<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ï¸Ž Running: CLAHE + TVF filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clahe_tvf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1354/1354 [04:47<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ï¸Ž Running: CLAHE + GDF filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clahe_gdf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1354/1354 [04:44<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    image_ext = {\".png\", \".jpg\", \".jpeg\", \".bmp\"}\n",
    "    img_paths = [p for p in INPUT_ROOT.rglob(\"*\") if p.suffix.lower() in image_ext]\n",
    "    print(f\"Found {len(img_paths)} images in {INPUT_ROOT}\")\n",
    "\n",
    "    for flt in FILTER_KEYS:\n",
    "        print(f\"\\nâ–¶ï¸Ž Running: CLAHE + {flt.upper()} filter\")\n",
    "        for img_path in tqdm(img_paths, desc=f\"clahe_{flt}\"):\n",
    "            process_image(img_path, flt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
